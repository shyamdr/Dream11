{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7153f525-daf8-40cb-985b-49f42a68820e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a26ef43b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T09:59:40.206381Z",
     "start_time": "2024-01-20T09:59:40.193680Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import modules.psql as psql\n",
    "from sqlalchemy import types as altypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b8138",
   "metadata": {},
   "source": [
    "# Postgres Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1710c3bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T09:59:40.625570Z",
     "start_time": "2024-01-20T09:59:40.543705Z"
    }
   },
   "outputs": [],
   "source": [
    "%run config_psql.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3773860a",
   "metadata": {},
   "source": [
    "# Settings Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "33156a11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T09:59:40.909733Z",
     "start_time": "2024-01-20T09:59:40.894197Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Settings configurations\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f847205",
   "metadata": {},
   "source": [
    "# Initializing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "84a070f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T09:59:41.223079Z",
     "start_time": "2024-01-20T09:59:41.193571Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://cricsheet.org/downloads/recently_played_30_json.zip\"\n",
    "filetype = \".json\"\n",
    "\n",
    "df_meta = pd.DataFrame()\n",
    "df_match = pd.DataFrame()\n",
    "df_official = pd.DataFrame()\n",
    "df_registry = pd.DataFrame()\n",
    "df_player = pd.DataFrame()\n",
    "df_innings = pd.DataFrame()\n",
    "df_deliveries = pd.DataFrame()\n",
    "df_powerplay = pd.DataFrame()\n",
    "df_absent_hurt = pd.DataFrame()\n",
    "df_miscounted_overs = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8554acca",
   "metadata": {},
   "source": [
    "# Read the downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c3f64acd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T09:59:45.901126Z",
     "start_time": "2024-01-20T09:59:41.522905Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    content = response.content\n",
    "    \n",
    "    zip_file = zipfile.ZipFile(io.BytesIO(content))\n",
    "    \n",
    "    with zip_file.open('README.txt') as f:\n",
    "        lines = [line.decode('utf-8') for line in f.readlines()]\n",
    "        pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2}) - ([^-]+) - ([^-]+) - (\\w+) - (\\d+) - (.+)')\n",
    "        ids = [match.group(5) for line in lines if (match := pattern.match(line))]\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa23f9",
   "metadata": {},
   "source": [
    "# Building indivudal DataFrames for different tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "36ce7cd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T10:00:02.843737Z",
     "start_time": "2024-01-20T09:59:45.903223Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125  files present\n",
      "1386130 executed!\n",
      "1388217 executed!\n",
      "1389397 executed!\n",
      "1392657 executed!\n",
      "1409496 executed!\n",
      "1409528 executed!\n",
      "1412542 executed!\n",
      "1386128 executed!\n",
      "1386129 executed!\n",
      "1392655 executed!\n",
      "1392656 executed!\n",
      "1409495 executed!\n",
      "1409527 executed!\n",
      "1386127 executed!\n",
      "1388216 executed!\n",
      "1392654 executed!\n",
      "1409494 executed!\n",
      "1409526 executed!\n",
      "1386126 executed!\n",
      "1389396 executed!\n",
      "1392653 executed!\n",
      "1409493 executed!\n",
      "1409525 executed!\n",
      "1412541 executed!\n",
      "1386125 executed!\n",
      "1409492 executed!\n",
      "1409524 executed!\n",
      "1386124 executed!\n",
      "1406083 executed!\n",
      "1409491 executed!\n",
      "1409523 executed!\n",
      "1386123 executed!\n",
      "1409490 executed!\n",
      "1409522 executed!\n",
      "1412540 executed!\n",
      "1386122 executed!\n",
      "1406082 executed!\n",
      "1409489 executed!\n",
      "1409521 executed!\n",
      "1386121 executed!\n",
      "1409488 executed!\n",
      "1409520 executed!\n",
      "1412539 executed!\n",
      "1386120 executed!\n",
      "1406081 executed!\n",
      "1409487 executed!\n",
      "1386119 executed!\n",
      "1409486 executed!\n",
      "1409518 executed!\n",
      "1375844 executed!\n",
      "1386117 executed!\n",
      "1386118 executed!\n",
      "1387604 executed!\n",
      "1409485 executed!\n",
      "1409517 executed!\n",
      "1386116 executed!\n",
      "1406080 executed!\n",
      "1409484 executed!\n",
      "1409516 executed!\n",
      "1412551 executed!\n",
      "1386114 executed!\n",
      "1409483 executed!\n",
      "1409515 executed!\n",
      "1386113 executed!\n",
      "1388215 executed!\n",
      "1412550 executed!\n",
      "1386112 executed!\n",
      "1406079 executed!\n",
      "1409482 executed!\n",
      "1409514 executed!\n",
      "1386111 executed!\n",
      "1388214 executed!\n",
      "1412549 executed!\n",
      "1386110 executed!\n",
      "1406078 executed!\n",
      "1409480 executed!\n",
      "1409512 executed!\n",
      "1386109 executed!\n",
      "1388213 executed!\n",
      "1409479 executed!\n",
      "1409511 executed!\n",
      "1375843 executed!\n",
      "1386107 executed!\n",
      "1386108 executed!\n",
      "1387603 executed!\n",
      "1409478 executed!\n",
      "1409510 executed!\n",
      "1412535 executed!\n",
      "1412533 executed!\n",
      "1412534 executed!\n",
      "1386105 executed!\n",
      "1386106 executed!\n",
      "1388212 executed!\n",
      "1398260 executed!\n",
      "1412531 executed!\n",
      "1412532 executed!\n",
      "1386104 executed!\n",
      "1409476 executed!\n",
      "1409508 executed!\n",
      "1412530 executed!\n",
      "1373583 executed!\n",
      "1386103 executed!\n",
      "1387602 executed!\n",
      "1406077 executed!\n",
      "1409475 executed!\n",
      "1409507 executed!\n",
      "1386102 executed!\n",
      "1388211 executed!\n",
      "1398259 executed!\n",
      "1373582 executed!\n",
      "1386101 executed!\n",
      "1387601 executed!\n",
      "1409474 executed!\n",
      "1409506 executed!\n",
      "1388201 executed!\n",
      "1387600 executed!\n",
      "1408102 executed!\n",
      "1408108 executed!\n",
      "1411276 executed!\n",
      "1373581 executed!\n",
      "1398258 executed!\n",
      "1411273 executed!\n",
      "1411274 executed!\n",
      "1375842 executed!\n",
      "1406076 executed!\n"
     ]
    }
   ],
   "source": [
    "print(len(ids), \" files present\")\n",
    "#for file in ids[-40:]:\n",
    "\n",
    "for file in ids:\n",
    "    with zip_file.open(file+filetype) as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "        # -----------------------------\n",
    "        # DataFrame to store - Metadata\n",
    "        df_meta = pd.concat([df_meta, pd.DataFrame([data[\"meta\"]]).assign(filename=file, filetype=filetype)])\n",
    "        \n",
    "        # ----------------------------------\n",
    "        # DataFrame to store - match details\n",
    "        df_info = pd.DataFrame([data[\"info\"]])\n",
    "        df_match_temp = pd.concat([\n",
    "            pd.json_normalize(df_info['event'], sep='_').add_prefix('event_'),\n",
    "            pd.DataFrame(df_info[list(set(['balls_per_over','season', 'gender', 'city', 'venue', 'match_type', 'match_type_number', 'overs', 'team_type']) & set(df_info.columns))]),\n",
    "            df_info['dates'].apply(lambda x: [x[0], x[-1]]).apply(pd.Series).rename(columns={0: 'start_date', 1: 'end_date'}),\n",
    "            df_info['teams'].apply(lambda x: [x[0], x[1]]).apply(pd.Series).rename(columns={0: 'team_host', 1: 'team_visitor'}),\n",
    "            pd.json_normalize(df_info['toss'], sep='_').add_prefix('toss_'),\n",
    "            pd.json_normalize(df_info['outcome'], sep='_').add_prefix('outcome_')\n",
    "        ], axis=1).assign(match_id = file)\n",
    "        if 'player_of_match' in df_info.columns:\n",
    "            df_match_temp['player_of_match'] = df_info['player_of_match'].apply(lambda x: ','.join(x))\n",
    "            \n",
    "        df_match = pd.concat([df_match, df_match_temp])\n",
    "        \n",
    "        # -----------------------------------\n",
    "        # DataFrame to store official details\n",
    "        df_umpire = pd.json_normalize(df_info['officials'], sep = '_')\n",
    "        umpire_set = set()\n",
    "        \n",
    "        for column in df_umpire.columns:\n",
    "            umpire_set.update(df_umpire[column].explode().dropna())\n",
    "\n",
    "        df_umpire2 = pd.DataFrame(index=list(umpire_set), columns=df_umpire.columns).fillna(False)\n",
    "        \n",
    "        for column in df_umpire.columns:\n",
    "            df_umpire2[column] = df_umpire2.index.isin(df_umpire[column].explode().dropna())\n",
    "            \n",
    "        df_umpire2 = df_umpire2.reset_index().rename(columns={'index': 'name'}).assign(match_id = file)     \n",
    "        df_official = pd.concat([df_official, df_umpire2])\n",
    "\n",
    "        # -------------------------------------\n",
    "        # DataFrame to store - registry details\n",
    "        df_registry = pd.concat([\n",
    "            df_registry,\n",
    "            pd.DataFrame(list(data[\"info\"][\"registry\"][\"people\"].items()), columns=['people', 'identifier']).assign(match_id = file)\n",
    "        ])\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # DataFrame to store - match player details\n",
    "        df_player = pd.concat([\n",
    "            df_player,\n",
    "            pd.json_normalize(df_info['players']).melt(var_name='team', value_name='player').explode('player').assign(match_id = file)\n",
    "        ])\n",
    "        \n",
    "        # -------------------------------------------------\n",
    "        # DataFrame to store - innings details\n",
    "        df_innings = pd.concat([df_innings,\n",
    "                                pd.json_normalize(data['innings'], sep = '_').drop('overs', axis = 1).assign(match_id = file)])\n",
    "        \n",
    "        # -------------------------------------------------\n",
    "        # DataFrame to store - deliveries ball by ball\n",
    "        for i in data['innings']:\n",
    "            index = data['innings'].index(i)\n",
    "            df_deliveries = pd.concat([df_deliveries,\n",
    "                                        pd.json_normalize(i,\n",
    "                                                          record_path=['overs', 'deliveries'],\n",
    "                                                          meta=['team', ['overs', 'over']],\n",
    "                                                          sep='_'\n",
    "                                                         )\n",
    "                                        .assign(match_id=file, inning = index+1)\n",
    "                                       ])\n",
    "        \n",
    "        # --------------------------------------\n",
    "        # DataFrame to store - powerplay details\n",
    "        for i in data['innings']:\n",
    "            if 'powerplays' in pd.json_normalize(i).columns:\n",
    "                index = data['innings'].index(i)\n",
    "                df_powerplay = pd.concat([df_powerplay, pd.json_normalize(data['innings'][index], record_path = ['powerplays'], meta = ['team'], sep = '_').assign(match_id = file)])\n",
    "\n",
    "            if 'absent_hurt' in pd.json_normalize(i).columns:\n",
    "                index = data['innings'].index(i)\n",
    "                df_absent_hurt = pd.concat([\n",
    "                    df_absent_hurt,\n",
    "                    pd.json_normalize(data['innings'][index])[['team','absent_hurt']].explode('absent_hurt').assign(match_id = file)\n",
    "                ])                \n",
    "\n",
    "        df_miscounted_overs = pd.concat([df_miscounted_overs,pd.DataFrame([\n",
    "            {\n",
    "                \"team\": inning.get(\"team\", \"\"),\n",
    "                \"miscounted_over\": over_number,\n",
    "                \"balls\": over_data.get(\"balls\", \"\"),\n",
    "                \"umpire\": over_data.get(\"umpire\", \"\")\n",
    "            }\n",
    "            for inning in data.get(\"innings\", [])\n",
    "            for over_number, over_data in inning.get(\"miscounted_overs\", {}).items()\n",
    "        ]).assign(match_id = file)])\n",
    "        \n",
    "    print(file + \" executed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1082a5a",
   "metadata": {},
   "source": [
    "## Adding/Modifying additional fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7704b307",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T10:00:02.915924Z",
     "start_time": "2024-01-20T10:00:02.844010Z"
    }
   },
   "outputs": [],
   "source": [
    "match_id_list = \", \".join([f\"'{match_id}'\" for match_id in ids])\n",
    "\n",
    "df_meta['created'] = pd.to_datetime(df_meta['created'])\n",
    "\n",
    "# Merging registry details into match-player details\n",
    "df_player.reset_index(inplace = True, drop = True)\n",
    "df_registry.reset_index(inplace = True, drop = True)\n",
    "df_player.rename(columns = {'player':'name'}, inplace = True)\n",
    "df_player['player_id'] = df_player.merge(df_registry, how='left', left_on=['match_id', 'name'], right_on=['match_id', 'people'])['identifier']\n",
    "\n",
    "df_official.reset_index(inplace = True, drop = True)\n",
    "df_official['official_id'] = df_official.merge(df_registry, how='left', left_on=['match_id','name'], right_on=['match_id','people'])['identifier']\n",
    "\n",
    "df_miscounted_overs.reset_index(inplace = True, drop = True)\n",
    "\n",
    "df_innings.drop(['powerplays','absent_hurt'], axis=1, inplace=True, errors='ignore')\n",
    "df_innings.drop(df_innings.filter(like='miscounted_overs_').columns, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "if not df_absent_hurt.empty:\n",
    "    df_absent_hurt.reset_index(inplace = True, drop = True)\n",
    "    df_absent_hurt.rename(columns = {'absent_hurt':'name'}, inplace = True)\n",
    "    df_absent_hurt['player_id'] = df_absent_hurt.merge(df_registry, how='left', left_on=['match_id', 'name'], right_on=['match_id', 'people'])['identifier']\n",
    "    \n",
    "df_deliveries.reset_index(inplace = True, drop = True)\n",
    "df_deliveries.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2548c616",
   "metadata": {},
   "source": [
    "## Load data into Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5569cd",
   "metadata": {},
   "source": [
    "#### 1. Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e8906782",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T10:00:04.094255Z",
     "start_time": "2024-01-20T10:00:02.925368Z"
    }
   },
   "outputs": [],
   "source": [
    "# Upsert MetaData information\n",
    "query = psql.upsert(\n",
    "    engine,\n",
    "    dataFrame = df_meta,\n",
    "    table = \"meta\",\n",
    "    schema = \"dwh\",\n",
    "    pk_col = list(df_meta.columns),\n",
    "    update_col = list(df_meta.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b9bf05",
   "metadata": {},
   "source": [
    "#### 2. Officials(umpires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "61d86674",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T10:00:04.856637Z",
     "start_time": "2024-01-20T10:00:04.103272Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load official(umpires) information\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(f\"DELETE FROM dwh.official WHERE match_id IN ({match_id_list})\")\n",
    "    \n",
    "count_rows = df_official.to_sql('official', schema='dwh', con=engine, if_exists='append', method='multi', index=False)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(\"\"\"\n",
    "        UPDATE dwh.official OF\n",
    "        SET is_registered = FALSE\n",
    "        FROM dwh.people P\n",
    "        WHERE OF.name = P.identifier AND P.identifier IS NULL;\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.execute(\"\"\"\n",
    "        UPDATE dwh.official off\n",
    "        SET official_id_num = p.id\n",
    "        FROM dwh.people p\n",
    "        WHERE off.official_id = p.identifier\n",
    "        AND official_id_num IS NULL;\n",
    "    \"\"\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a38d81",
   "metadata": {},
   "source": [
    "#### 3. Player-match (players who played a particular match) & Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "112c2bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T10:00:05.707408Z",
     "start_time": "2024-01-20T10:00:04.856637Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load match_player information into Stage table\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(\"TRUNCATE TABLE stg.match_player\")\n",
    "\n",
    "count_rows = df_player.to_sql('match_player', schema = 'stg', con = engine, if_exists='append', method = 'multi', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "419810c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T10:00:05.782994Z",
     "start_time": "2024-01-20T10:00:05.713007Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load match information into dwh layer\n",
    "with engine.connect() as conn:\n",
    "    conn.execution_options(isolation_level = \"AUTOCOMMIT\")\n",
    "    with conn.begin():\n",
    "        conn.execute(\"CALL dwh.LoadMatchPlayerAndTeam()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1907f590",
   "metadata": {},
   "source": [
    "#### 4. Match details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0d3d2fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T10:00:06.302371Z",
     "start_time": "2024-01-20T10:00:05.787810Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load match information into Stage table\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(\"TRUNCATE TABLE stg.match\")\n",
    "\n",
    "count_rows = df_match.to_sql('match', schema = 'stg', con = engine, if_exists='append', method = 'multi', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "66c5ed1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T10:00:06.401152Z",
     "start_time": "2024-01-20T10:00:06.302884Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load match information into dwh layer\n",
    "with engine.connect() as conn:\n",
    "    conn.execution_options(isolation_level = \"AUTOCOMMIT\")\n",
    "    with conn.begin():\n",
    "        conn.execute(\"CALL dwh.LoadMatch()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5225c",
   "metadata": {},
   "source": [
    "#### 5. absent hurt details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fa7eded2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T10:00:06.863315Z",
     "start_time": "2024-01-20T10:00:06.412891Z"
    }
   },
   "outputs": [],
   "source": [
    "query = psql.insert_without_duplicate(\n",
    "    engine,\n",
    "    dataFrame = df_absent_hurt,\n",
    "    table = \"absent_hurt\",\n",
    "    schema = \"dwh\",\n",
    "    conflict_col = list(df_absent_hurt.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b190a5ee",
   "metadata": {},
   "source": [
    "#### 6. miscounted overs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "019b5937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T10:00:07.383898Z",
     "start_time": "2024-01-20T10:00:06.865825Z"
    }
   },
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    conn.execute(f\"DELETE FROM dwh.miscounted_over WHERE match_id IN ({match_id_list})\")\n",
    "    \n",
    "count_rows = df_miscounted_overs.to_sql('miscounted_over', schema='dwh', con=engine, if_exists='append', method='multi', index=False)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(\"\"\"\n",
    "        UPDATE dwh.miscounted_over MO\n",
    "        SET umpire = NULL\n",
    "        WHERE umpire = ''\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.execute(\"\"\"\n",
    "        UPDATE dwh.miscounted_over MO\n",
    "        SET umpire_id_num = official_id_num\n",
    "        FROM dwh.official OFF\n",
    "        WHERE MO.match_id = OFF.match_id AND MO.umpire = OFF.name AND MO.umpire_id_num IS NULL;\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90291d99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T16:08:10.084299Z",
     "start_time": "2023-12-03T16:08:10.034595Z"
    }
   },
   "source": [
    "#### 7. innings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ecaacffd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T10:00:07.821167Z",
     "start_time": "2024-01-20T10:00:07.389660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    conn.execute(\"TRUNCATE TABLE stg.inning\")\n",
    "    \n",
    "df_innings.to_sql('inning', schema = 'stg', con = engine, if_exists='append', method = 'multi', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7e5fec3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T10:00:07.917990Z",
     "start_time": "2024-01-20T10:00:07.827913Z"
    }
   },
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    conn.execution_options(isolation_level = \"AUTOCOMMIT\")\n",
    "    with conn.begin():\n",
    "        conn.execute(\"CALL dwh.LoadInning()\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb0563a",
   "metadata": {},
   "source": [
    "#### 8. powerplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a0aca76d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T10:00:08.297207Z",
     "start_time": "2024-01-20T10:00:07.923330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    conn.execute(\"TRUNCATE TABLE stg.powerplay\")\n",
    "\n",
    "df_powerplay.to_sql('powerplay', schema = 'stg', con = engine, if_exists='append', method = 'multi', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a0be5aef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T10:00:08.401140Z",
     "start_time": "2024-01-20T10:00:08.304220Z"
    }
   },
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    conn.execution_options(isolation_level = \"AUTOCOMMIT\")\n",
    "    with conn.begin():\n",
    "        conn.execute(\"CALL dwh.LoadPowerplay()\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed483dd0",
   "metadata": {},
   "source": [
    "#### 9. delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "963eb83d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T10:01:01.905707Z",
     "start_time": "2024-01-20T10:00:08.405390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41684"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    conn.execute(\"TRUNCATE TABLE stg.delivery\")\n",
    "\n",
    "df_deliveries.to_sql('delivery', \n",
    "                     schema = 'stg', \n",
    "                     con = engine, \n",
    "                     if_exists='append', \n",
    "                     method = 'multi', \n",
    "                     dtype = {\n",
    "                                 \"wickets\":altypes.JSON(none_as_null=True),\n",
    "                                 \"replacements_match\":altypes.JSON(none_as_null=True),\n",
    "                                 \"replacements_role\":altypes.JSON(none_as_null=True)\n",
    "                             },\n",
    "                     index = False,\n",
    "                     chunksize = 5000)\n",
    "\n",
    "## 692 records per second | chunksize = 10k\n",
    "# 1076 records per second | chunksize = 2k\n",
    "# 1050 records per second | chunksize = 1024\n",
    "# 1015 records per second | chunksize = 1k\n",
    "# 1005 records per second | chunksize = 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a30e53f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T10:01:04.333318Z",
     "start_time": "2024-01-20T10:01:01.914106Z"
    }
   },
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    conn.execution_options(isolation_level = \"AUTOCOMMIT\")\n",
    "    with conn.begin():\n",
    "        conn.execute(\"CALL dwh.LoadDelivery()\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "156.042px",
    "left": "1382.44px",
    "right": "20px",
    "top": "120px",
    "width": "315.417px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
