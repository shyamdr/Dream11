{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2330813",
   "metadata": {},
   "source": [
    "# Importing  Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f82435a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T05:58:15.120580Z",
     "start_time": "2023-09-17T05:58:15.104910Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1ec63",
   "metadata": {},
   "source": [
    "# Settings configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a16820f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T05:58:15.837012Z",
     "start_time": "2023-09-17T05:58:15.822796Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "comm_filepath = os.path.abspath(os.path.join(os.getcwd(), \"../../../data/CommExtract/\"))\n",
    "pitch_score_filepath = os.path.abspath(os.path.join(os.getcwd(), \"../../../config/pitchAI/score/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5002a9ca",
   "metadata": {},
   "source": [
    "# Initializing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7687a9d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T05:58:18.134765Z",
     "start_time": "2023-09-17T05:58:16.842651Z"
    }
   },
   "outputs": [],
   "source": [
    "comm_file = '\\\\comm_69561.txt'\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30afa326",
   "metadata": {},
   "source": [
    "# Load the commentary & scoring files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604ee71e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T05:58:19.652806Z",
     "start_time": "2023-09-17T05:58:19.312687Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    # Normalize the text to remove special characters and normalize spaces\n",
    "    normalized_text = unicodedata.normalize('NFKD', text)\n",
    "    # Remove special characters and control characters\n",
    "    cleaned_text = ''.join(c for c in normalized_text if not unicodedata.category(c).startswith('C'))\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Load the list from the text file\n",
    "comm0 = []\n",
    "with open(comm_filepath + comm_file, 'r') as f:\n",
    "    for line in f:\n",
    "        cleaned_line = remove_special_characters(line)\n",
    "        comm0.append(cleaned_line)\n",
    "        \n",
    "df_RS = pd.read_excel(pitch_score_filepath + '\\\\relevance_score.xlsx')\n",
    "df_KS = pd.read_excel(pitch_score_filepath + '\\\\keyword_score.xlsx')\n",
    "df_Comp = pd.read_excel(pitch_score_filepath + '\\\\comparative_score.xlsx')\n",
    "df_Neg = pd.read_excel(pitch_score_filepath + '\\\\negative_score.xlsx')\n",
    "#df['Word'] = df['Word'].apply(lambda x : nlp(x)[0].lemma_.lower())\n",
    "df_RS['Word'] = df_RS['Word'].apply(lambda x : x.lower() if isinstance(x, str) else x)\n",
    "df_KS['Word'] = df_KS['Word'].apply(lambda x : x.lower() if isinstance(x, str) else x)\n",
    "df_KS['Helper'] = df_KS['Helper'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "df_Comp['Word'] = df_Comp['Word'].apply(lambda x : x.lower() if isinstance(x, str) else x)\n",
    "df_Neg['Word'] = df_Neg['Word'].apply(lambda x : x.lower() if isinstance(x, str) else x)\n",
    "df_Score = pd.DataFrame({'Keyword': [], 'Score': [], 'Bat': [], 'Pace': [], 'Spin': []})\n",
    "\n",
    "df_RS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c56101",
   "metadata": {},
   "source": [
    "# Filter 1 : Extract only pre-match commentary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a620a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T05:59:01.843572Z",
     "start_time": "2023-09-17T05:59:01.812905Z"
    }
   },
   "outputs": [],
   "source": [
    "ball_num = ['0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '1.1','1.2','1.3','1.4','1.5','1.6']\n",
    "max_ind = 0\n",
    "for i in range(len(comm0)-1,-1,-1 ):\n",
    "    if comm0[i] in ball_num:\n",
    "        if i > max_ind:\n",
    "            max_ind = i\n",
    "print(max_ind)\n",
    "comm = comm0[max_ind+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d45bb60",
   "metadata": {},
   "source": [
    "# Filter 2 : Filter out non alpha-numeric charecter sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e4912",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:22:13.977305Z",
     "start_time": "2023-09-09T19:22:13.959415Z"
    }
   },
   "outputs": [],
   "source": [
    "text_comm = [element for element in comm if any(char.isalpha() for char in element)]\n",
    "\n",
    "print('the original comm file contains - ' + str(len(comm)) + ' lines of text')\n",
    "print('after filtering for alpha-numeric texts, the file contains - ' + str(len(text_comm)) + ' lines of text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b7a0b6",
   "metadata": {},
   "source": [
    "# Function : POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a9527a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:22:14.427099Z",
     "start_time": "2023-09-09T19:22:14.411459Z"
    }
   },
   "outputs": [],
   "source": [
    "def func_pos_tag(arr):\n",
    "    pos_tag = []\n",
    "    for text in arr:\n",
    "        doc = nlp(text)\n",
    "        pos_count = {}\n",
    "        for token in doc:\n",
    "            pos = token.pos_\n",
    "            if pos in pos_count:\n",
    "                pos_count[pos] += 1\n",
    "            else:\n",
    "                pos_count[pos] = 1\n",
    "        pos_tag.append(pos_count)\n",
    "    return pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba1b7e",
   "metadata": {},
   "source": [
    "# Filter 3 : Filter out squad, playing XI etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519670c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:22:15.747791Z",
     "start_time": "2023-09-09T19:22:14.749874Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_tag = func_pos_tag(text_comm)\n",
    "\n",
    "text_comm2 = []\n",
    "pos_tag2 = []\n",
    "\n",
    "for i, text in enumerate(text_comm):\n",
    "    pos_count = pos_tag[i]\n",
    "    noun_count = pos_count.get('PROPN', 0)\n",
    "    punctuation_count = pos_count.get('PUNCT', 0)\n",
    "    total_count = sum(pos_count.values())\n",
    "\n",
    "    if (noun_count + punctuation_count) / total_count <= 0.6:\n",
    "        text_comm2.append(text)\n",
    "        #pos_tag2.append(pos_tag[i])\n",
    "#text_comm2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93684fd",
   "metadata": {},
   "source": [
    "# Filter 4 : Filter relevant texts based on average length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bdda37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:22:15.763450Z",
     "start_time": "2023-09-09T19:22:15.752424Z"
    }
   },
   "outputs": [],
   "source": [
    "def func_find_threshold(num_words):\n",
    "    # Sort the num_words list in descending order\n",
    "    sorted_num_words = sorted(num_words, reverse=True)\n",
    "    \n",
    "    # Define the sample size as a percentage of the total data points\n",
    "    sample_size_percent = 0.4  # Change this value as needed\n",
    "    \n",
    "    # Calculate the number of textlines to consider based on the sample size percentage\n",
    "    num_records = int(len(sorted_num_words) * sample_size_percent)\n",
    "    \n",
    "    # Calculate the avg_topn as the average of the top n records\n",
    "    #n = int(sample_size_percent * 100)  # Change this value as needed\n",
    "    avg_topn = sum(sorted_num_words[:num_records]) / num_records\n",
    "    \n",
    "    # Create a new list to store data points above the threshold\n",
    "    above_threshold = [num for num in sorted_num_words if num >= avg_topn * 0.7]\n",
    "    \n",
    "    # Calculate the average of the above_threshold list\n",
    "    average_above_threshold = sum(above_threshold) / len(above_threshold)\n",
    "    \n",
    "    print(\"Average of the top\", int(sample_size_percent * 100) , \"% records:\", avg_topn, \"no. of words/textline\")\n",
    "    print(\"Word counts of textlines\", int(sample_size_percent * 100), \"% or above the avg:\", above_threshold)\n",
    "    print(\"Word count average of the modified file now is :\", average_above_threshold, \" words/textline\")\n",
    "    \n",
    "    return min(above_threshold)\n",
    "\n",
    "# Calculate the number of words in each text line\n",
    "num_words = [len(text.split()) for text in text_comm2]\n",
    "threshold = func_find_threshold(num_words)\n",
    "text_comm3 = [element for element in text_comm2 if len(element.split()) >= threshold]\n",
    "#text_comm3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce7fcb4",
   "metadata": {},
   "source": [
    "## Tokenize paragraph to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9857a5b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:22:15.783578Z",
     "start_time": "2023-09-09T19:22:15.766358Z"
    }
   },
   "outputs": [],
   "source": [
    "line_comm = []\n",
    "for paragraph in text_comm3:\n",
    "    sentences = nltk.tokenize.sent_tokenize(paragraph)\n",
    "    line_comm.append(sentences)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea63f2",
   "metadata": {},
   "source": [
    "## Perform Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a97dcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:22:16.501735Z",
     "start_time": "2023-09-09T19:22:15.882962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a copy of line_comm with stop words removed and lemmatization applied\n",
    "lemmatized_comm = []\n",
    "for sentences in line_comm:\n",
    "    lemmatized_sentences = []\n",
    "    for sentence in sentences:        \n",
    "        doc = nlp(sentence)\n",
    "        lemmatized_tokens = [token.lemma_.lower() for token in doc if not token.is_stop]        \n",
    "        lemmatized_tokens = [word.strip(string.punctuation) for word in lemmatized_tokens if word.strip(string.punctuation)]\n",
    "        lemmatized_sentences.append(\" \".join(lemmatized_tokens)) if lemmatized_tokens else None\n",
    "    lemmatized_comm.append(lemmatized_sentences)\n",
    "#lemmatized_comm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10553c3",
   "metadata": {},
   "source": [
    "## Extracting relevant text - Relevance Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c59656c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:22:18.317996Z",
     "start_time": "2023-09-09T19:22:16.501735Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_score(text):\n",
    "    words = text.split()  # Split the text into individual words\n",
    "    matched_words = []  # Initialize a list to store matched words or combinations\n",
    "    scores = []  # Initialize a list to store scores for matched words or combinations\n",
    "    \n",
    "    # Calculate scores and track matched words for unigrams\n",
    "    for i in range(len(words)):\n",
    "        unigram = set([words[i]])\n",
    "\n",
    "        for index,row in df_RS[(df_RS['Usage'] == 'AddScore') & (df_RS['Sequence'] == 'Unigram')].iterrows():\n",
    "            if unigram == set(row['Word'].split()):\n",
    "                score = row['Score']\n",
    "                scores.append(score)\n",
    "                matched_words.append(unigram)\n",
    "                \n",
    "    # Calculate scores and track matched words for bigrams\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram = set([words[i], words[i+1]])\n",
    "\n",
    "        for index,row in df_RS[(df_RS['Usage'] == 'AddScore') & (df_RS['Sequence'] == 'Bigram')].iterrows():\n",
    "            if bigram == set(row['Word'].split()):\n",
    "                score = row['Score'] * 1.5\n",
    "                scores.append(score)\n",
    "\n",
    "    # Calculate scores and track matched words for trigrams\n",
    "    for i in range(len(words) - 2):\n",
    "        bigram = set([words[i], words[i+1], words[i+2]])\n",
    "\n",
    "        for index,row in df_RS[(df_RS['Usage'] == 'AddScore') & (df_RS['Sequence'] == 'Trigram')].iterrows():\n",
    "            if bigram == set(row['Word'].split()):\n",
    "                score = row['Score'] * 2\n",
    "                scores.append(score)\n",
    "                \n",
    "    return sum(scores), matched_words\n",
    "\n",
    "# Calculate the total score, matched words, and average score for each element in lemmatized_comm\n",
    "score_list = []\n",
    "matched_words_list = []\n",
    "avg_score_list = []\n",
    "score_list_2d = []  # Initialize a list to store 2D scores for each line\n",
    "\n",
    "for para in lemmatized_comm:\n",
    "    para_score = 0\n",
    "    para_matched_words = []\n",
    "    para_word_count = 0\n",
    "    para_score_list = []\n",
    "\n",
    "    for line in para:\n",
    "        score, matched_words = calculate_score(line)\n",
    "        para_score += score\n",
    "        para_word_count += len(line.split())\n",
    "        para_matched_words.extend(matched_words)\n",
    "        para_score_list.append(score)\n",
    "\n",
    "    score_list.append(para_score)\n",
    "    matched_words_list.append(para_matched_words)\n",
    "    avg_score_list.append(round((para_score / para_word_count), 3))\n",
    "    score_list_2d.append(para_score_list)\n",
    "\n",
    "print(\"Total Scores:\", score_list, sep = '\\n', end = '\\n\\n')\n",
    "print(\"Matched Words:\", matched_words_list, sep = '\\n', end = '\\n\\n')\n",
    "print(\"Average Scores:\", avg_score_list, sep = '\\n', end = '\\n\\n')\n",
    "print(\"2D Scores:\", score_list_2d, sep = '\\n', end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c6c13a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:22:18.333318Z",
     "start_time": "2023-09-09T19:22:18.323605Z"
    }
   },
   "outputs": [],
   "source": [
    "line_comm2 = []\n",
    "max_avg = max(avg_score_list)\n",
    "print(\"Cutoff - \", max_avg * 0.35)\n",
    "for i in range(len(avg_score_list)):\n",
    "    if avg_score_list[i] >= 0.35 * max_avg:\n",
    "        for j in range(len(score_list_2d[i])):\n",
    "            if score_list_2d[i][j] != 0:\n",
    "                sentences = re.split(r'(?<=[.,!?])\\s', line_comm[i][j])\n",
    "                line_comm2.extend(sentences)\n",
    "                #line_comm2.append(line_comm[i][j])\n",
    "# for i in line_comm2:\n",
    "#     print(i)\n",
    "line_comm2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c59269",
   "metadata": {},
   "source": [
    "# Key-Word Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd46b1",
   "metadata": {},
   "source": [
    "### 1. Lemmatized data with negative and comparatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff074935",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:22:18.495980Z",
     "start_time": "2023-09-09T19:22:18.336303Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatized_comm2 = []\n",
    "comp_list = list(df_Comp['Word'])\n",
    "neg_list = list(df_Neg['Word'])\n",
    "for sentence in line_comm2:\n",
    "# for sentence in [\"There is not a lot of dew in this surface to be expected in today's game.\",\n",
    "#                  \"Grass is nowhere to be seen. Pitch is flat, Huge total to be expected here today\",\n",
    "#                 \"This is a high scoring venue\"]:\n",
    "    doc = nlp(sentence)\n",
    "    lemmatized_tokens = [token.lemma_.lower() for token in doc if not token.is_stop \n",
    "                         or token.lemma_.lower() in neg_list \n",
    "                         or token.lemma_.lower() in comp_list]\n",
    "\n",
    "    lemmatized_tokens = [word.strip(string.punctuation) for word in lemmatized_tokens if word.strip(string.punctuation)]\n",
    "    lemmatized_comm2.append(\" \".join(lemmatized_tokens)) if lemmatized_tokens else None\n",
    "lemmatized_comm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b368b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:22:18.511611Z",
     "start_time": "2023-09-09T19:22:18.499590Z"
    }
   },
   "outputs": [],
   "source": [
    "KS_word_list = list(df_KS['Word'].unique())\n",
    "KS_helper_list = list(df_KS['Helper'].unique())\n",
    "comp_list = list(df_Comp['Word'].unique())\n",
    "neg_list = list(df_Neg['Word'].unique())\n",
    "col_list = ['keyword','score','bat','pace','spin','negative_flag']\n",
    "dtype_dict = {'keyword': 'object', \n",
    "          'score': 'float64', \n",
    "          'bat': 'float64', \n",
    "          'pace': 'float64', \n",
    "          'spin':'float64', \n",
    "          'negative_flag': 'bool'}\n",
    "df_KWScore = pd.DataFrame(columns = col_list).astype(dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b5882",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:22:18.558740Z",
     "start_time": "2023-09-09T19:22:18.513607Z"
    }
   },
   "outputs": [],
   "source": [
    "keyword_chunks = []\n",
    "for line in lemmatized_comm2:\n",
    "    #print(\"the line is - \", line)\n",
    "    \n",
    "    line_words = line.split()\n",
    "    for i, word in enumerate(line_words):\n",
    "        if (word in KS_word_list):\n",
    "            keyword = [word]\n",
    "            comp_ind_list = []\n",
    "            neg_ind_list = []\n",
    "            helper_found = False\n",
    "            left_index, right_index = i,i\n",
    "            left_index_found, right_index_found = False, False\n",
    "            neg_in_keyword = False\n",
    "            \n",
    "            j = i-1            \n",
    "            while (j>=0 and not left_index_found):                \n",
    "                if ((df_KS['Word'] == word) & (df_KS['Helper'] == line_words[j])).any() and not helper_found:\n",
    "                    helper_found = True\n",
    "                    left_index -= 1\n",
    "                elif (line_words[j] in comp_list):\n",
    "                    left_index -= 1\n",
    "                elif (line_words[j] in neg_list):\n",
    "                    left_index -= 1\n",
    "                    neg_in_keyword = not neg_in_keyword\n",
    "                else:\n",
    "                    left_index_found = True\n",
    "                j -= 1\n",
    "\n",
    "            j = i+1\n",
    "            while (j < len(line_words) and not right_index_found):\n",
    "                if (((df_KS['Word'] == word) & (df_KS['Helper'] == line_words[j])).any() and not helper_found):\n",
    "                    helper_found = True\n",
    "                    right_index += 1\n",
    "                elif (line_words[j] in comp_list):\n",
    "                    right_index += 1\n",
    "                elif (line_words[j] in neg_list):\n",
    "                    right_index += 1\n",
    "                    neg_in_keyword = not neg_in_keyword\n",
    "                else:\n",
    "                    right_index_found = True \n",
    "                j += 1\n",
    "            keyword_chunks.append([word, line_words[left_index:right_index+1]])\n",
    "keyword_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e2030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:22:18.573368Z",
     "start_time": "2023-09-09T19:22:18.562494Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_category_score(score, index):\n",
    "    # Get the row at the specified index\n",
    "    row = df_KS.iloc[index]\n",
    "    nan = float('nan')\n",
    "    # Initialize scores for each category as blank by default\n",
    "    scores = {'Bat': nan, 'Pace': nan, 'Spin': nan}\n",
    "\n",
    "    # Define column weights\n",
    "    weights = {'TypeBenifit1': 1, 'TypeBenifit2': 0.7, 'TypeDeficit1': -1, 'TypeDeficit2': -0.7}\n",
    "\n",
    "    # Iterate over column names and their corresponding weights\n",
    "    for col_name, weight in weights.items():\n",
    "        if not pd.isna(row[col_name]):\n",
    "            categories = row[col_name].split(',')\n",
    "            for category in categories:\n",
    "                category = category.strip()  # Remove leading/trailing spaces\n",
    "                if category in scores:\n",
    "                    scores[category] = weight * score\n",
    "\n",
    "    return scores['Bat'], scores['Pace'], scores['Spin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab65cf43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:22:18.604881Z",
     "start_time": "2023-09-09T19:22:18.575937Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_keyword_score(keyword = None, arr = []):\n",
    "    \n",
    "\n",
    "    chunk = arr\n",
    "    #Step 1 : find index of keyword & helper\n",
    "    helper = -1\n",
    "    \n",
    "    #print(keyword, chunk)\n",
    "    \n",
    "    for word in chunk:\n",
    "        if keyword == word:\n",
    "            continue\n",
    "        if (word in KS_helper_list) & (df_KS[((df_KS['Word'] == keyword) & (df_KS['Helper'] == word))]['Score'].any()):\n",
    "            KW_Score = float(df_KS[((df_KS['Word'] == keyword) & (df_KS['Helper'] == word))]['Score'])\n",
    "            KS_ind = df_KS.index[(df_KS['Word'] == keyword) & (df_KS['Helper'] == word)].tolist()\n",
    "            helper = word\n",
    "            #chunk.remove(word)\n",
    "    if helper == -1:\n",
    "        KW_Score = float(df_KS[((df_KS['Word'] == keyword) & (df_KS['Helper'] == -1))]['Score'])\n",
    "        KS_ind = df_KS.index[(df_KS['Word'] == keyword) & (df_KS['Helper'] == -1)].tolist()\n",
    "        \n",
    "        \n",
    "    #Step 2 : find all comparatives\n",
    "    Comp_Score = 0.0\n",
    "    Comp_Score_Mul = 1.0\n",
    "    for word in chunk:\n",
    "        if (keyword == word) or (helper == word):\n",
    "            continue            \n",
    "        if word in comp_list:\n",
    "            if ((df_Comp['Word'] == word) & (df_Comp['Type'] == 'Add')).any(): # Adding Comp Score\n",
    "                Comp_Score = float(df_Comp[df_Comp['Word'] == word]['Score'])\n",
    "                #chunk.remove(word)\n",
    "                \n",
    "            if ((df_Comp['Word'] == word) & (df_Comp['Type'] == 'Mul')).any(): # Multiplying Comp Score\n",
    "                Comp_Score_Mul *= float(df_Comp[df_Comp['Word'] == word]['Score'])\n",
    "                #chunk.remove(word)\n",
    "                \n",
    "    #Step 3 : find all negatives\n",
    "    neg_state = False\n",
    "    for word in chunk:\n",
    "        if (keyword == word) or (helper == word):\n",
    "            continue\n",
    "        if word in neg_list:\n",
    "            neg_state = not neg_state\n",
    "            \n",
    "    #Step 4 : Find total score\n",
    "    Total_Score = 0 if neg_state else (KW_Score + Comp_Score) * Comp_Score_Mul\n",
    "    bat_score, pace_score, spin_score = calculate_category_score(Total_Score, KS_ind[0])\n",
    "    diff_score = [bat_score, pace_score, spin_score]\n",
    "    \n",
    "    return Total_Score, diff_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb57de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:22:18.812436Z",
     "start_time": "2023-09-09T19:22:18.673939Z"
    }
   },
   "outputs": [],
   "source": [
    "df_Score = pd.DataFrame({'Keyword': [], 'Score': [], 'Bat': [], 'Pace': [], 'Spin': []})\n",
    "\n",
    "def custom_sort_key(row):\n",
    "    return (\n",
    "        abs(row['Score']),\n",
    "        abs(row['Bat']),\n",
    "        abs(row['Pace']),\n",
    "        abs(row['Spin'])\n",
    "    )\n",
    "\n",
    "for obj in keyword_chunks:\n",
    "    keyword = ' '.join(obj[1])\n",
    "    #print(obj[0],obj[1])\n",
    "    score, diff_score = calculate_keyword_score(obj[0], obj[1])\n",
    "    #print(x, '|', score, '|', diff_score)\n",
    "    if score != 0.0:\n",
    "        df_Score.loc[len(df_Score)] = [keyword, score, diff_score[0], diff_score[1], diff_score[2]]   \n",
    "        \n",
    "\n",
    "df_Score['Rank'] = df_Score.groupby('Keyword', group_keys=False).apply(\n",
    "    lambda x: x.apply(custom_sort_key, axis=1).rank(method='dense', ascending=False))\n",
    "df_Score.drop(df_Score[df_Score['Rank'] != 1].index, inplace=True)\n",
    "df_Score.drop(columns=['Rank'], inplace=True)\n",
    "\n",
    "Score_count = [df_Score['Bat'].count(),\n",
    "               df_Score['Pace'].count(),\n",
    "               df_Score['Spin'].count()]\n",
    "Score_mean = [df_Score['Bat'].mean(),\n",
    "              df_Score['Pace'].mean(),\n",
    "              df_Score['Spin'].mean(),]\n",
    "\n",
    "#------------------Type - 1 --------------------\n",
    "# Score_rating = [\n",
    "#     float(\"{:.2f}\".format(float(Score_mean[0] * 5 / Score_count[0]))) if Score_count[0] > 3 else nan,\n",
    "#     float(\"{:.2f}\".format(float(Score_mean[1] * 5 / Score_count[1]))) if Score_count[1] > 3 else nan,\n",
    "#     float(\"{:.2f}\".format(float(Score_mean[2] * 5 / Score_count[2]))) if Score_count[2] > 3 else nan]\n",
    "\n",
    "# Actual_Score_rating = [float(\"{:.2f}\".format(float(Score_mean[0] * 5 / Score_count[0]))),\n",
    "#                        float(\"{:.2f}\".format(float(Score_mean[1] * 5 / Score_count[1]))),\n",
    "#                        float(\"{:.2f}\".format(float(Score_mean[2] * 5 / Score_count[2])))]\n",
    "#----------------------------------------------\n",
    "\n",
    "Score_rating = [\n",
    "    float(\"{:.2f}\".format(float(Score_mean[0] * 5 / 3 ))) if Score_count[0] > 3 else nan,\n",
    "    float(\"{:.2f}\".format(float(Score_mean[1] * 5 / 3 ))) if Score_count[1] > 3 else nan,\n",
    "    float(\"{:.2f}\".format(float(Score_mean[2] * 5 / 3 ))) if Score_count[2] > 3 else nan,\n",
    "]\n",
    "\n",
    "Actual_Score_rating = [float(\"{:.2f}\".format(float(Score_mean[0] * 5 / 3 ))),\n",
    "                       float(\"{:.2f}\".format(float(Score_mean[1] * 5 / 3 ))),\n",
    "                       float(\"{:.2f}\".format(float(Score_mean[2] * 5 / 3 ))),]\n",
    "\n",
    "print(Score_rating, sep = '\\n')\n",
    "print(Actual_Score_rating, sep = '\\n')\n",
    "df_Score.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cad091",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:22:18.888653Z",
     "start_time": "2023-09-09T19:22:18.869575Z"
    }
   },
   "outputs": [],
   "source": [
    "descriptions = []\n",
    "ranges = [\n",
    "    (-np.inf, -6, \"Nightmare\"),\n",
    "    (-6, -4, \"Challenging\"),\n",
    "    (-4, -2, \"Slightly disadvantageous\"),\n",
    "    (-2, 2, \"Balanced\"),\n",
    "    (2, 4, \"Slightly favourable\"),\n",
    "    (4, 6, \"Highly favourable\"),\n",
    "    (6, np.inf, \"Paradise\")]\n",
    "\n",
    "for score in Score_rating:\n",
    "    description = \"Unknown\"\n",
    "    for lower, upper, desc in ranges:\n",
    "        if lower <= score <= upper:\n",
    "            description = desc\n",
    "            break\n",
    "    descriptions.append(\"Insufficient Data\") if np.isnan(score) else descriptions.append(description)\n",
    "print(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e203a95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:22:19.079248Z",
     "start_time": "2023-09-09T19:22:19.046176Z"
    }
   },
   "outputs": [],
   "source": [
    "df_Score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29123847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc7430c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf89a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f07251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1bfb45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dee633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cee46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e26d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b801c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db645a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dc7a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9644fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eaa0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b9c25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee64114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0dd2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c58ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eee6d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef1861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f781e612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec7bf83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3467def8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dfd416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972046d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f49fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d73c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c498c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ab628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e0bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a894c8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3998237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81084ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd1e31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa1b2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0292fbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bf323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe715e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b8ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = [\n",
    "    (-np.inf, -3.75, \"Nightmare\"),\n",
    "    (-3.75, -2.5, \"Challenging\"),\n",
    "    (-2.5, -1.25, \"Slightly disadvantageous\"),\n",
    "    (-1.25, 1.25, \"Balanced\"),\n",
    "    (1.25, 2.5, \"Slightly favourable\"),\n",
    "    (2.5, 3.75, \"Highly favourable\"),\n",
    "    (3.75, np.inf, \"Paradise\"),\n",
    "    (np.nan, np.nan, \"Insufficient data\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678e4cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T20:40:03.432150Z",
     "start_time": "2023-08-06T20:40:03.279787Z"
    }
   },
   "outputs": [],
   "source": [
    "for line in final_comm:\n",
    "    doc = nlp(line)\n",
    "    for token in doc:\n",
    "        print([token, token.pos_, spacy.explain(token.pos_)])\n",
    "    print(end = '\\n')\n",
    "    \n",
    "\"\"\"\n",
    "little dry\n",
    "want win\n",
    "give chance\n",
    "need play\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b61569b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T20:42:01.106515Z",
     "start_time": "2023-08-06T20:42:01.074655Z"
    }
   },
   "outputs": [],
   "source": [
    "word = \"bounce\"\n",
    "\n",
    "text1 = \"\"\"It is a little dry as well.\"\"\"\n",
    "text2 = \"\"\"Winning gives us a chance\"\"\"\n",
    "\n",
    "doc = nlp(text1)\n",
    "for token in doc:\n",
    "    print([token, token.pos_, spacy.explain(token.pos_)])\n",
    "    \n",
    "# doc = nlp(text2)\n",
    "# for token in doc:\n",
    "#     if token.text == word:\n",
    "#         print([token, token.pos_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd73577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e42d370",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T09:59:59.993827Z",
     "start_time": "2023-09-07T09:59:59.961169Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"fewer\")\n",
    "for token in doc:\n",
    "    print(token.lemma_.lower())\n",
    "\n",
    "for token in doc:\n",
    "    print(token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0a8d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T06:36:17.680092Z",
     "start_time": "2023-09-07T06:36:17.662177Z"
    }
   },
   "outputs": [],
   "source": [
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc362b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa1713f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c328c42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T08:02:48.980847Z",
     "start_time": "2023-09-09T08:02:48.954025Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"This one though looks like a beautiful batting wicket.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd5b584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59307a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T08:03:07.350174Z",
     "start_time": "2023-09-09T08:03:07.333915Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"batting\")\n",
    "for token in doc:\n",
    "    print(token.lemma_.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb2b28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c95d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4463044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaeeecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ae05e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fad00bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be0524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T09:49:52.175888Z",
     "start_time": "2023-09-09T09:49:52.169894Z"
    }
   },
   "outputs": [],
   "source": [
    "line_comm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be4b68a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T09:52:40.328166Z",
     "start_time": "2023-09-09T09:52:40.155107Z"
    }
   },
   "outputs": [],
   "source": [
    "for line in line_comm2:\n",
    "    doc = nlp(line)\n",
    "    #doc = nlp(\"He ought to improve his footwork if he wants to become a better batsman\")\n",
    "\n",
    "    print(doc,end = '\\n')\n",
    "    #doc = nlp(\"The pitch will behave differently run in second innings\")\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.tag_ in (\"VB\"):\n",
    "            continue\n",
    "        elif i >= 0 and token.pos_ == \"VERB\" and doc[i-1].pos_ == \"AUX\":\n",
    "            continue\n",
    "        elif i >= 0 and token.pos_ in (\"VERB\", \"AUX\") and doc[i-1].pos_ == \"ADV\" and doc[i-2].pos_ == \"AUX\":\n",
    "            continue  \n",
    "        elif token.pos_ in (\"VERB\", \"AUX\"):\n",
    "            print([token, token.pos_, spacy.glossary.GLOSSARY[token.tag_], token.tag_])\n",
    "        \n",
    "                      \n",
    "# get list of all modal and auxillary verbs from nltk and get the desired tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352676c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889c9832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04bbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c4022a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f067456c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2039e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c0d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49bf53d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T06:24:53.177700Z",
     "start_time": "2023-09-07T06:24:53.168925Z"
    }
   },
   "outputs": [],
   "source": [
    "text_comm4 = []\n",
    "lemmatized_comm_para_filtered = []\n",
    "lemmatized_comm_line_filtered = []\n",
    "final_comm = []\n",
    "max_avg = max(avg_score_list)\n",
    "for i in range(len(avg_score_list)):\n",
    "    if avg_score_list[i] >= 0.35 * max_avg:\n",
    "        text_comm4.append(text_comm3[i])\n",
    "        lemmatized_comm_para_filtered.append(lemmatized_comm[i])\n",
    "        \n",
    "        for j in range(len(score_list_2d[i])):\n",
    "            if score_list_2d[i][j] != 0: #columns\n",
    "                lemmatized_comm_line_filtered.append(lemmatized_comm[i][j])\n",
    "                final_comm.append(line_comm[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565df07c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cd770e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878ec49d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e5960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4618aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d84835d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758b81ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0bc86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba7c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50c541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3a8f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T13:42:55.659646Z",
     "start_time": "2023-08-06T13:42:55.640031Z"
    }
   },
   "outputs": [],
   "source": [
    "max_avg * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0329b02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T12:45:12.332163Z",
     "start_time": "2023-08-06T12:45:12.319985Z"
    }
   },
   "outputs": [],
   "source": [
    "text_comm4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c068186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8812c8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d68f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717f9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91895e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157deb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc6c6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T12:37:43.846751Z",
     "start_time": "2023-08-06T12:37:43.600530Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,2.5))\n",
    "sns.histplot(score_list, kde = True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (5,2.5))\n",
    "sns.histplot(avg_score_list, kde = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03691efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9d2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c7985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ffa13d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee2a30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88a1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4492d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f39f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af61a93c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243b71b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956f05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6299c5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d36686b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a79e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b018a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f1f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b94396",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T07:30:21.183544Z",
     "start_time": "2023-08-06T07:30:21.163072Z"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    # Normalize the text to remove special characters and normalize spaces\n",
    "    normalized_text = unicodedata.normalize('NFKD', text)\n",
    "    # Remove special characters and control characters\n",
    "    cleaned_text = ''.join(c for c in normalized_text if not unicodedata.category(c).startswith('C'))\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "\n",
    "comm = []\n",
    "with open(FILENAME, 'r') as f:\n",
    "    for line in f:\n",
    "        cleaned_line = remove_special_characters(line)\n",
    "        comm.append(cleaned_line)\n",
    "\n",
    "# Now 'comm' contains the lines with all special characters removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb57647",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T07:34:47.007305Z",
     "start_time": "2023-08-06T07:34:46.999071Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "68m on one side and 63m on the other square boundary, the straight boundary is at 78m. Spinners would be licking their lips in the dressing room. It's dry and will definitely turn. There are patches which will offer plenty to the quality\\xa0spinners in both sides. It's dry and should become slow as the match progresses. 170 could be a match-winning total. Bat first, runs on the board in a final will matter, reckons\n",
    "\"\"\"\n",
    "unicodedata.normalize('NFKD', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332fdc47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956c7f8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T07:31:50.062440Z",
     "start_time": "2023-08-06T07:31:50.039102Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ee202",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T20:18:23.416812Z",
     "start_time": "2023-08-05T20:18:23.401953Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy.explain(\"AUX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcabc7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T20:18:44.421148Z",
     "start_time": "2023-08-05T20:18:44.395688Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy.POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae7acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a851569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e80290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e18f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a80503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e77ce37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a172d25b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6f87d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6dad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532fb2af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8ba159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df389ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2edb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f074bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8590e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fbeab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb2c0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a36c132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcdfcf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99daed92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e6277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fd4171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669516f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac98452",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T16:26:03.665926Z",
     "start_time": "2023-08-05T16:26:02.761233Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Calculate the number of words in each text line\n",
    "num_words = [len(text.split()) for text in text_comm]\n",
    "\n",
    "# Plotting the distribution of the number of words\n",
    "sns.histplot(num_words, kde=True)\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Original Distribution of Number of Words in each text line')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Calculate the average number of alphabetical characters in each word\n",
    "#avg_alphabet_chars = [sum(len(word) for word in text.split() if word.isalpha()) / len(text.split()) for text in filter_comm]\n",
    "avg_alphabet_chars = [sum(len(re.findall('[a-zA-Z]', word)) for word in textline.split()) / len(textline.split()) for textline in text_comm2]\n",
    "\n",
    "# Plotting the distribution of the average number of alphabetical characters\n",
    "sns.histplot(avg_alphabet_chars, kde=True)\n",
    "plt.xlabel('Average Number of Alphabetical Characters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Average Number of Alphabetical Characters in Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b7e9af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T16:23:04.012442Z",
     "start_time": "2023-08-05T16:23:03.991907Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad3090f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277902b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T16:15:43.634089Z",
     "start_time": "2023-08-05T16:15:43.604510Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_num_words = sorted(num_words, reverse=True)\n",
    "\n",
    "num_records = int(len(num_words) * 0.5)\n",
    "#num_records #56\n",
    "\n",
    "avg_topn = sum(sorted_num_words[:num_records]) / num_records\n",
    "avg_topn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446dbf24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T16:27:56.473472Z",
     "start_time": "2023-08-05T16:27:56.443582Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_alphabet_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815c2c6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T16:29:48.798417Z",
     "start_time": "2023-08-05T16:29:48.773457Z"
    }
   },
   "outputs": [],
   "source": [
    "text_comm2[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392a542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157b818",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:23.492296Z",
     "start_time": "2023-07-19T12:19:23.190141Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Calculate the number of words in each text line\n",
    "num_words = [len(text.split()) for text in text_comm]\n",
    "\n",
    "# Plotting the distribution of the number of words\n",
    "sns.histplot(num_words, kde=True)\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Original Distribution of Number of Words in each text line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b6cc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:23.731784Z",
     "start_time": "2023-07-19T12:19:23.496133Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Calculate the number of words in each text line\n",
    "num_words = [len(text.split()) for text in text_comm]\n",
    "\n",
    "# Plotting the distribution of the number of words\n",
    "sns.histplot(num_words, kde=True)\n",
    "\n",
    "# Calculate the cumulative distribution function (CDF)\n",
    "values, base = np.histogram(num_words, bins='auto')\n",
    "cumulative = np.cumsum(values) / sum(values)\n",
    "\n",
    "# Find the index where the CDF crosses the specified threshold\n",
    "x = 0.1\n",
    "idx = np.argmax(cumulative >= x)\n",
    "\n",
    "# Mark the threshold line on the histogram\n",
    "plt.axvline(x=base[idx + 1], color='red', linestyle='--', label=f'{x * 100}% Area')\n",
    "\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Original Distribution of Number of Words in each text line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d18d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a3292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6d10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ec96a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a85ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f67b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c480a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d631a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e320fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26135287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82afb06f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ee9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2b836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fce3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c7ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d1e340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18de5abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b028c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:25.565677Z",
     "start_time": "2023-07-19T12:19:25.236542Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Calculate the number of words in each text line\n",
    "num_words = [len(text.split()) for text in text_comm]\n",
    "\n",
    "# Plotting the distribution of the number of words\n",
    "sns.histplot(num_words, kde=True)\n",
    "\n",
    "# Calculate the cumulative distribution function (CDF)\n",
    "values, bins, _ = plt.hist(num_words, bins='auto', density=True, cumulative=True)\n",
    "cumulative = values * np.diff(bins)\n",
    "\n",
    "# Find the value where the cumulative probability crosses the specified threshold\n",
    "x = 0.7\n",
    "idx = np.argmax(cumulative >= x)\n",
    "\n",
    "# Get the corresponding value from the bins array\n",
    "threshold_value = bins[idx]\n",
    "\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Original Distribution of Number of Words in each text line')\n",
    "plt.show()\n",
    "\n",
    "print(f\"The value that creates {x * 100}% of the area is: {threshold_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce8a82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:25.581137Z",
     "start_time": "2023-07-19T12:19:25.566705Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define your array of numbers\n",
    "data = np.array([0,1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12,13,14,15,16,17,18,19,20])\n",
    "\n",
    "# Create the histogram\n",
    "hist, bins = np.histogram(data, bins='auto', density=True)\n",
    "\n",
    "# Compute the cumulative distribution function (CDF)\n",
    "cdf = np.cumsum(hist * np.diff(bins))\n",
    "\n",
    "# Find the value where the area under the curve becomes 70%\n",
    "target_area = 0.3\n",
    "index = np.searchsorted(cdf, target_area)\n",
    "\n",
    "# Get the corresponding value from the bins array\n",
    "value = bins[index]\n",
    "\n",
    "# Print the result\n",
    "print(f\"The value where the area under the histogram curve becomes 70% is {value}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d9c32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa463bfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:25.739451Z",
     "start_time": "2023-07-19T12:19:25.583789Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data array\n",
    "data_array = np.arange(20, -1, -1)\n",
    "\n",
    "# Calculate the area of the original triangle\n",
    "original_triangle_area = 0.5 * 20 * 20\n",
    "\n",
    "# User-inputted area\n",
    "#area = float(input(\"Enter the area (as a fraction of the original triangle area): \"))\n",
    "area = 0.8\n",
    "\n",
    "# Calculate the target area for the small triangle\n",
    "target_area = area * original_triangle_area\n",
    "\n",
    "# Calculate the cumulative sum of the data array\n",
    "cumulative_sum = np.cumsum(data_array)\n",
    "\n",
    "# Find the index where the cumulative sum crosses the target area\n",
    "index = np.where(cumulative_sum >= target_area)[0][0]\n",
    "\n",
    "# Calculate the remaining area needed\n",
    "remaining_area = target_area - cumulative_sum[index-1]\n",
    "\n",
    "# Calculate the fraction of remaining area in relation to the data point difference\n",
    "fraction = remaining_area / data_array[index]\n",
    "\n",
    "# Calculate the value of x\n",
    "x = index + fraction\n",
    "\n",
    "# Plotting the triangle\n",
    "plt.plot(data_array, 'b', linewidth=2)\n",
    "plt.fill_between(range(index+1), data_array[:index+1], color='blue', alpha=0.5)\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Triangle\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print the value of x\n",
    "print(\"The value of x is:\", x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d82c202",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:25.962077Z",
     "start_time": "2023-07-19T12:19:25.740259Z"
    }
   },
   "outputs": [],
   "source": [
    "arr = [len(text.split()) for text in text_comm]\n",
    "area = 0.8\n",
    "\n",
    "# Creating the histogram and KDE plot using seaborn\n",
    "sns.histplot(arr, kde=True, stat='density', color='blue')\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Histogram and KDE\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Calculate the cumulative distribution function (CDF)\n",
    "n, bins = np.histogram(arr, bins='auto', density=True)\n",
    "cdf = np.cumsum(n * np.diff(bins))\n",
    "\n",
    "x = np.interp(area, cdf, bins[:-1])\n",
    "\n",
    "plt.axvline(x=x, color='red', linestyle='--', label=f'x = {x:.2f}')\n",
    "\n",
    "print(\"The value of x is:\", x)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3a06d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6eec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcbd5d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:25.994004Z",
     "start_time": "2023-07-19T12:19:25.966088Z"
    }
   },
   "outputs": [],
   "source": [
    "comm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac97b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b8652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac218ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4dee7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b2126f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e0f007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ac3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801e821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5641132e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424add59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e17f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:25.996015Z",
     "start_time": "2023-07-19T12:19:25.996015Z"
    }
   },
   "outputs": [],
   "source": [
    "text_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745fccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef5ff47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ec8528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6320591a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5496c95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36e9785f",
   "metadata": {},
   "source": [
    "# POS analysis and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ebc27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:26.122273Z",
     "start_time": "2023-07-19T12:19:26.106977Z"
    }
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# pos_tags = []\n",
    "# text = \"After 70 league games spread over a period of a littleover 7 weeks, we finally have four survivors contesting to take home the prized IPL trophy. Will MI be 6th time champs? Can CSK grab a 5th title? Will Gujarat Titans continue their winning spree and defend their title or will Lucknow Super Giantsbecome a first time champions? Lot to play for as IPL 2023 reaches the home stretch with today's Qualifier 1. The Titans cruised through the league phase once again and were comfortably the best team on display, they just didn't confirm their place in the playoffs, but they did so emphatically. On the other hand, the Super Kings had to wait until their final game of the league, but they do have the knack of winning big moments, especially when playing at home. The winner of tonight's game will go through to Sunday's final while the loser will wait for tomorrow's winner in QF 2 on 26th in Ahmedabad. CSK not just have home advantage tonight, but they're the better rested team, having played their last league match on Saturday. On the other hand, Titans's last match spilled into the wee hours of yesterday and they'd to travel as well. Would that be a telling factor today?\"\n",
    "\n",
    "# doc = nlp(text)\n",
    "# tags = [(token.text, token.pos_) for token in doc]\n",
    "# tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047fc27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:27.483545Z",
     "start_time": "2023-07-19T12:19:26.156493Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "pos_counts = []\n",
    "\n",
    "for text in text_comm2:\n",
    "    doc = nlp(text)\n",
    "    pos_count = {}\n",
    "\n",
    "    for token in doc:\n",
    "        pos = token.pos_\n",
    "        if pos in pos_count:\n",
    "            pos_count[pos] += 1\n",
    "        else:\n",
    "            pos_count[pos] = 1\n",
    "\n",
    "    pos_counts.append(pos_count)\n",
    "# pos_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3be90fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:28.291448Z",
     "start_time": "2023-07-19T12:19:27.486203Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract unique POS tags\n",
    "unique_pos_tags = sorted(set([pos for pos_count in pos_counts for pos in pos_count.keys()]))\n",
    "\n",
    "# Create an array to store the counts for each POS tag\n",
    "pos_counts_array = np.zeros((len(pos_counts), len(unique_pos_tags)))\n",
    "\n",
    "# Fill in the array with the counts\n",
    "for i, pos_count in enumerate(pos_counts):\n",
    "    for j, pos_tag in enumerate(unique_pos_tags):\n",
    "        pos_counts_array[i, j] = pos_count.get(pos_tag, 0)\n",
    "\n",
    "# Plotting the stacked bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(pos_counts)), pos_counts_array.T[0], label=unique_pos_tags[0])\n",
    "\n",
    "for i in range(1, len(unique_pos_tags)):\n",
    "    plt.bar(range(len(pos_counts)), pos_counts_array.T[i], bottom=np.sum(pos_counts_array.T[:i], axis=0),\n",
    "            label=unique_pos_tags[i])\n",
    "\n",
    "plt.xlabel('Line of Text')\n",
    "plt.ylabel('Count')\n",
    "plt.title('POS Tag Distribution for Each Line of Text')\n",
    "plt.xticks(range(len(pos_counts)), range(1, len(pos_counts) + 1))\n",
    "#plt.xticks(range(60,73), range(61,74))\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ccfd4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:28.307338Z",
     "start_time": "2023-07-19T12:19:28.291448Z"
    }
   },
   "outputs": [],
   "source": [
    "text_comm3 = []\n",
    "\n",
    "for i, text in enumerate(text_comm2):\n",
    "    pos_count = pos_counts[i]\n",
    "    noun_count = pos_count.get('PROPN', 0)\n",
    "    punctuation_count = pos_count.get('PUNCT', 0)\n",
    "    total_count = sum(pos_count.values())\n",
    "\n",
    "    if (noun_count + punctuation_count) / total_count <= 0.6:\n",
    "        text_comm3.append(text)\n",
    "text_comm3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc76e72a",
   "metadata": {},
   "source": [
    "# Calculating word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fb60b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:29.646483Z",
     "start_time": "2023-07-19T12:19:28.309827Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "word_counts = []\n",
    "for i in range(len(text_comm3)):\n",
    "    doc = nlp(text_comm3[i])\n",
    "    # Calculate word frequencies, excluding stop words\n",
    "    word_frequencies = {}\n",
    "    for token in doc:\n",
    "        word = token.text.lower()\n",
    "        if word.isalpha() and word not in STOP_WORDS:\n",
    "            if word not in word_frequencies:\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "\n",
    "    all_word_frequencies = [(word, freq) for word, freq in word_frequencies.items()]\n",
    "    #print('received values for line' + str(i))\n",
    "    word_counts.append(all_word_frequencies)\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd7c811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:29.662297Z",
     "start_time": "2023-07-19T12:19:29.646483Z"
    }
   },
   "outputs": [],
   "source": [
    "text_comm3[-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031876de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:29.678038Z",
     "start_time": "2023-07-19T12:19:29.662297Z"
    }
   },
   "outputs": [],
   "source": [
    "text_comm2[60:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf71c510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b45b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82942da2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852e315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de6c1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b43ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224ac8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfaef3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e8c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d70de4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a15c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8596f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3240fe2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26be8f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1270b4a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:29.693937Z",
     "start_time": "2023-07-19T12:19:29.678038Z"
    }
   },
   "outputs": [],
   "source": [
    "num_words = [len(text.split()) for text in text_comm]\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb43cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:29.709871Z",
     "start_time": "2023-07-19T12:19:29.696445Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_value_for_area(data, area_percentage):\n",
    "    sorted_data = np.sort(data)\n",
    "    total_points = len(sorted_data)\n",
    "    rank = int((total_points * area_percentage) / 100)\n",
    "    \n",
    "    if rank == 0:\n",
    "        return sorted_data[0]\n",
    "    elif rank >= total_points:\n",
    "        return sorted_data[-1]\n",
    "    else:\n",
    "        rank_floor = int(np.floor(rank))\n",
    "        rank_decimal = rank - rank_floor\n",
    "        value = sorted_data[rank_floor - 1] + (rank_decimal * (sorted_data[rank_floor] - sorted_data[rank_floor - 1]))\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2988430",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:29.725884Z",
     "start_time": "2023-07-19T12:19:29.711878Z"
    }
   },
   "outputs": [],
   "source": [
    "area = 99.999999999999999999999999999999999999999\n",
    "value = calculate_value_for_area(num_words, area)\n",
    "print(f\"The value for {area}% area below the curve is: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ebe66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd801f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b49d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f5ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce039924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd4ca7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0befb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:19:37.596384Z",
     "start_time": "2023-07-19T12:19:37.566613Z"
    }
   },
   "outputs": [],
   "source": [
    "text_comm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6a035b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c85caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae246ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a52fb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb57ef2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de5d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ecd31c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342cb49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5fc5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa1dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c76a15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492198d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6610a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2cdc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10df9cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9b906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a6fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93636d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1d2543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
