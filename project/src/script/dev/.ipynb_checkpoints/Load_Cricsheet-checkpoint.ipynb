{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85b9ee8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "a26ef43b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:11:59.835237Z",
     "start_time": "2023-12-03T13:11:59.824707Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import modules.psql as psql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161dd558",
   "metadata": {},
   "source": [
    "## Postgres Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "1710c3bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:12:00.311604Z",
     "start_time": "2023-12-03T13:12:00.229968Z"
    }
   },
   "outputs": [],
   "source": [
    "%run config_psql.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44a0253",
   "metadata": {},
   "source": [
    "## Settings Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "33156a11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:12:00.560840Z",
     "start_time": "2023-12-03T13:12:00.545584Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Settings configurations\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8985800",
   "metadata": {},
   "source": [
    "## Initializing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "84a070f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:28:01.328936Z",
     "start_time": "2023-12-03T13:28:01.307249Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://cricsheet.org/downloads/recently_played_30_json.zip\"\n",
    "filetype = \".json\"\n",
    "\n",
    "df_meta = pd.DataFrame()\n",
    "df_match = pd.DataFrame()\n",
    "df_official = pd.DataFrame()\n",
    "df_registry = pd.DataFrame()\n",
    "df_player = pd.DataFrame()\n",
    "df_innings = pd.DataFrame()\n",
    "df_deliveries = pd.DataFrame()\n",
    "df_powerplay = pd.DataFrame()\n",
    "df_absent_hurt = pd.DataFrame()\n",
    "df_miscounted_overs = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f357ab4d",
   "metadata": {},
   "source": [
    "## Read the downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "c3f64acd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:28:05.559863Z",
     "start_time": "2023-12-03T13:28:01.780457Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    content = response.content\n",
    "    \n",
    "    zip_file = zipfile.ZipFile(io.BytesIO(content))\n",
    "    \n",
    "    with zip_file.open('README.txt') as f:\n",
    "        lines = [line.decode('utf-8') for line in f.readlines()]\n",
    "        pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2}) - ([^-]+) - ([^-]+) - (\\w+) - (\\d+) - (.+)')\n",
    "        ids = [match.group(5) for line in lines if (match := pattern.match(line))]\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a981fc",
   "metadata": {},
   "source": [
    "## Building indivudal DataFrames for different tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "36ce7cd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:28:18.470150Z",
     "start_time": "2023-12-03T13:28:05.568123Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119  files present\n",
      "1389394 executed!\n",
      "1407107 executed!\n",
      "1407108 executed!\n",
      "1407109 executed!\n",
      "1407104 executed!\n",
      "1407105 executed!\n",
      "1407106 executed!\n",
      "1409206 executed!\n",
      "1389393 executed!\n",
      "1391785 executed!\n",
      "1391786 executed!\n",
      "1391787 executed!\n",
      "1407103 executed!\n",
      "1409205 executed!\n",
      "1407100 executed!\n",
      "1407101 executed!\n",
      "1407102 executed!\n",
      "1387226 executed!\n",
      "1387227 executed!\n",
      "1389392 executed!\n",
      "1407098 executed!\n",
      "1407099 executed!\n",
      "1387225 executed!\n",
      "1407096 executed!\n",
      "1407097 executed!\n",
      "1387223 executed!\n",
      "1387224 executed!\n",
      "1407094 executed!\n",
      "1407095 executed!\n",
      "1387222 executed!\n",
      "1389391 executed!\n",
      "1407092 executed!\n",
      "1407093 executed!\n",
      "1407719 executed!\n",
      "1387221 executed!\n",
      "1407089 executed!\n",
      "1407090 executed!\n",
      "1407091 executed!\n",
      "1407718 executed!\n",
      "1387219 executed!\n",
      "1387220 executed!\n",
      "1407716 executed!\n",
      "1407717 executed!\n",
      "1407714 executed!\n",
      "1407715 executed!\n",
      "1384439 executed!\n",
      "1387217 executed!\n",
      "1387218 executed!\n",
      "1407870 executed!\n",
      "1407871 executed!\n",
      "1387215 executed!\n",
      "1387216 executed!\n",
      "1391784 executed!\n",
      "1407864 executed!\n",
      "1407865 executed!\n",
      "1387213 executed!\n",
      "1387214 executed!\n",
      "1384438 executed!\n",
      "1387212 executed!\n",
      "1391783 executed!\n",
      "1407866 executed!\n",
      "1407867 executed!\n",
      "1384437 executed!\n",
      "1387210 executed!\n",
      "1387211 executed!\n",
      "1391782 executed!\n",
      "1407868 executed!\n",
      "1407869 executed!\n",
      "1387209 executed!\n",
      "1384436 executed!\n",
      "1387207 executed!\n",
      "1387208 executed!\n",
      "1384434 executed!\n",
      "1384435 executed!\n",
      "1387205 executed!\n",
      "1387206 executed!\n",
      "1384433 executed!\n",
      "1387203 executed!\n",
      "1387204 executed!\n",
      "1405126 executed!\n",
      "1384432 executed!\n",
      "1387202 executed!\n",
      "1384431 executed!\n",
      "1387199 executed!\n",
      "1387200 executed!\n",
      "1384430 executed!\n",
      "1405125 executed!\n",
      "1383568 executed!\n",
      "1384429 executed!\n",
      "1387198 executed!\n",
      "1391780 executed!\n",
      "1391781 executed!\n",
      "1384428 executed!\n",
      "1387196 executed!\n",
      "1387197 executed!\n",
      "1391779 executed!\n",
      "1405327 executed!\n",
      "1383566 executed!\n",
      "1383567 executed!\n",
      "1384426 executed!\n",
      "1384427 executed!\n",
      "1387194 executed!\n",
      "1387195 executed!\n",
      "1405124 executed!\n",
      "1384425 executed!\n",
      "1387192 executed!\n",
      "1387193 executed!\n",
      "1405325 executed!\n",
      "1405326 executed!\n",
      "1383562 executed!\n",
      "1383563 executed!\n",
      "1384424 executed!\n",
      "1387191 executed!\n",
      "1405321 executed!\n",
      "1405322 executed!\n",
      "1405323 executed!\n",
      "1405324 executed!\n",
      "1384423 executed!\n",
      "1387190 executed!\n"
     ]
    }
   ],
   "source": [
    "print(len(ids), \" files present\")\n",
    "for file in ids:\n",
    "    with zip_file.open(file+filetype) as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "        # -----------------------------\n",
    "        # DataFrame to store - Metadata\n",
    "        df_meta = pd.concat([df_meta, pd.DataFrame([data[\"meta\"]]).assign(filename=file, filetype=filetype)])\n",
    "        \n",
    "        # ----------------------------------\n",
    "        # DataFrame to store - match details\n",
    "        df_info = pd.DataFrame([data[\"info\"]])\n",
    "        df_match_temp = pd.concat([\n",
    "            pd.json_normalize(df_info['event'], sep='_').add_prefix('event_'),\n",
    "            pd.DataFrame(df_info[list(set(['balls_per_over','season', 'gender', 'city', 'venue', 'match_type', 'match_type_number', 'overs', 'team_type']) & set(df_info.columns))]),\n",
    "            df_info['dates'].apply(lambda x: [x[0], x[-1]]).apply(pd.Series).rename(columns={0: 'start_date', 1: 'end_date'}),\n",
    "            df_info['teams'].apply(lambda x: [x[0], x[1]]).apply(pd.Series).rename(columns={0: 'team_host', 1: 'team_visitor'}),\n",
    "            pd.json_normalize(df_info['toss'], sep='_').add_prefix('toss_'),\n",
    "            pd.json_normalize(df_info['outcome'], sep='_').add_prefix('outcome_')\n",
    "        ], axis=1).assign(match_id = file)\n",
    "        if 'player_of_match' in df_info.columns:\n",
    "            df_match_temp['player_of_match'] = df_info['player_of_match'].apply(lambda x: ','.join(x))\n",
    "            \n",
    "        df_match = pd.concat([df_match, df_match_temp])\n",
    "        \n",
    "        # -----------------------------------\n",
    "        # DataFrame to store official details\n",
    "        df_umpire = pd.json_normalize(df_info['officials'], sep = '_')\n",
    "        umpire_set = set()\n",
    "        \n",
    "        for column in df_umpire.columns:\n",
    "            umpire_set.update(df_umpire[column].explode().dropna())\n",
    "\n",
    "        df_umpire2 = pd.DataFrame(index=list(umpire_set), columns=df_umpire.columns).fillna(False)\n",
    "        \n",
    "        for column in df_umpire.columns:\n",
    "            df_umpire2[column] = df_umpire2.index.isin(df_umpire[column].explode().dropna())\n",
    "            \n",
    "        df_umpire2 = df_umpire2.reset_index().rename(columns={'index': 'name'}).assign(match_id = file)     \n",
    "        df_official = pd.concat([df_official, df_umpire2])\n",
    "\n",
    "        # -------------------------------------\n",
    "        # DataFrame to store - registry details\n",
    "        df_registry = pd.concat([\n",
    "            df_registry,\n",
    "            pd.DataFrame(list(data[\"info\"][\"registry\"][\"people\"].items()), columns=['people', 'identifier']).assign(match_id = file)\n",
    "        ])\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # DataFrame to store - match player details\n",
    "        df_player = pd.concat([\n",
    "            df_player,\n",
    "            pd.json_normalize(df_info['players']).melt(var_name='team', value_name='player').explode('player').assign(match_id = file)\n",
    "        ])\n",
    "        \n",
    "        # -------------------------------------------------\n",
    "        # DataFrame to store - innings ball-by-ball details\n",
    "        df_innings = pd.concat([df_innings,\n",
    "                                pd.json_normalize(data['innings'], sep = '_').drop('overs', axis = 1).assign(match_id = file)])\n",
    "        df_deliveries = pd.concat([df_deliveries, pd.json_normalize(data['innings'], record_path=['overs', 'deliveries'], meta=['team',['overs', 'over']], sep='_').assign(match_id = file)])\n",
    "        \n",
    "        # --------------------------------------\n",
    "        # DataFrame to store - powerplay details\n",
    "        for i in data['innings']:\n",
    "            if 'powerplays' in pd.json_normalize(i).columns:\n",
    "                index = data['innings'].index(i)\n",
    "                df_powerplay = pd.concat([df_powerplay, pd.json_normalize(data['innings'][index], record_path = ['powerplays'], meta = ['team'], sep = '_').assign(match_id = file)])\n",
    "\n",
    "            if 'absent_hurt' in pd.json_normalize(i).columns:\n",
    "                index = data['innings'].index(i)\n",
    "                df_absent_hurt = pd.concat([\n",
    "                    df_absent_hurt,\n",
    "                    pd.json_normalize(data['innings'][index])[['team','absent_hurt']].explode('absent_hurt').assign(match_id = file)\n",
    "                ])                \n",
    "\n",
    "        df_miscounted_overs = pd.concat([df_miscounted_overs,pd.DataFrame([\n",
    "            {\n",
    "                \"team\": inning.get(\"team\", \"\"),\n",
    "                \"miscounted_over\": over_number,\n",
    "                \"balls\": over_data.get(\"balls\", \"\"),\n",
    "                \"umpire\": over_data.get(\"umpire\", \"\")\n",
    "            }\n",
    "            for inning in data.get(\"innings\", [])\n",
    "            for over_number, over_data in inning.get(\"miscounted_overs\", {}).items()\n",
    "        ]).assign(match_id = file)])\n",
    "        \n",
    "    print(file + \" executed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eab8018",
   "metadata": {},
   "source": [
    "## Adding/Modifying additional fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "161b8ec7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:33:17.157585Z",
     "start_time": "2023-12-03T13:33:17.117784Z"
    }
   },
   "outputs": [],
   "source": [
    "match_id_list = \", \".join([f\"'{match_id}'\" for match_id in ids])\n",
    "\n",
    "df_meta['created'] = pd.to_datetime(df_meta['created'])\n",
    "\n",
    "# Merging registry details into match-player details\n",
    "df_player.reset_index(inplace = True, drop = True)\n",
    "df_registry.reset_index(inplace = True, drop = True)\n",
    "df_player.rename(columns = {'player':'name'}, inplace = True)\n",
    "df_player['player_id'] = df_player.merge(df_registry, how='left', left_on=['match_id', 'name'], right_on=['match_id', 'people'])['identifier']\n",
    "\n",
    "df_innings.drop('powerplays', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "if not df_absent_hurt.empty:\n",
    "    df_absent_hurt.reset_index(inplace = True, drop = True)\n",
    "    df_absent_hurt.rename(columns = {'absent_hurt':'name'}, inplace = True)\n",
    "    df_absent_hurt['player_id'] = df_absent_hurt.merge(df_registry, how='left', left_on=['match_id', 'name'], right_on=['match_id', 'people'])['identifier']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1084af1c",
   "metadata": {},
   "source": [
    "## Load data into Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a561c1",
   "metadata": {},
   "source": [
    "#### 1. Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7ba08444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T07:14:11.124707Z",
     "start_time": "2023-12-03T07:14:10.286872Z"
    }
   },
   "outputs": [],
   "source": [
    "# Upsert MetaData information\n",
    "query = psql.upsert(\n",
    "    engine,\n",
    "    dataFrame = df_meta,\n",
    "    table = \"meta\",\n",
    "    schema = \"dwh\",\n",
    "    pk_col = list(df_meta.columns),\n",
    "    update_col = list(df_meta.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5514e",
   "metadata": {},
   "source": [
    "#### 2. Officials(umpires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "08384295",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T06:39:50.895992Z",
     "start_time": "2023-12-03T06:39:49.693643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load official(umpires) information\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(f\"DELETE FROM dwh.official WHERE match_id IN ({match_id_list})\")\n",
    "    \n",
    "count_rows = df_official.to_sql('official', schema='dwh', con=engine, if_exists='append', method='multi', index=False)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(\"\"\"\n",
    "        UPDATE dwh.official OF\n",
    "        SET is_registered = FALSE\n",
    "        FROM dwh.people P\n",
    "        WHERE OF.name = P.identifier AND P.identifier IS NULL;\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc91bb",
   "metadata": {},
   "source": [
    "#### 3. Player-match (players who played a particular match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526f967",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T17:37:16.476262Z",
     "start_time": "2023-12-02T17:37:16.132761Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load player-match information\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(f\"DELETE FROM dwh.match_player WHERE match_id IN ({match_id_list})\")\n",
    "    \n",
    "count_rows = df_player.to_sql('match_player', schema='dwh', con=engine, if_exists='append', method='multi', index=False)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(\"\"\"\n",
    "        UPDATE dwh.match_player MP\n",
    "        SET is_registered = FALSE\n",
    "        FROM dwh.people P\n",
    "        WHERE MP.player_id = P.identifier AND P.identifier IS NULL;\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6273d33",
   "metadata": {},
   "source": [
    "#### 4. Match details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3d2fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T17:37:18.743236Z",
     "start_time": "2023-12-02T17:37:18.256078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load match information into Stage table\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(\"TRUNCATE TABLE stg.match\")\n",
    "\n",
    "count_rows = df_match.to_sql('match', schema = 'stg', con = engine, if_exists='append', method = 'multi', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc223e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T17:37:30.587142Z",
     "start_time": "2023-12-02T17:37:30.511035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load match information into dwh layer\n",
    "with engine.connect() as conn:\n",
    "    conn.execution_options(isolation_level = \"AUTOCOMMIT\")\n",
    "    with conn.begin():\n",
    "        conn.execute(\"CALL dwh.LoadMatch()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df856095",
   "metadata": {},
   "source": [
    "#### 5. absent hurt details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert absent hurt information\n",
    "query = psql.upsert(\n",
    "    engine,\n",
    "    dataFrame = df_meta,\n",
    "    table = \"meta\",\n",
    "    schema = \"dwh\",\n",
    "    pk_col = list(df_meta.columns),\n",
    "    update_col = list(df_meta.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758129c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c910d4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5388d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ee99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4625346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a792e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2daa7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "156.042px",
    "left": "1382.44px",
    "right": "20px",
    "top": "120px",
    "width": "315.417px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
